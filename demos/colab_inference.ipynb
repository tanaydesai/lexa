{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8MX4VvOJNun6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VzI7szuBN_O6"
      },
      "outputs": [],
      "source": [
        "configdict= {\n",
        "    \"gpt-1M\":{\n",
        "        \"batch_size\": 64,\n",
        "        \"block_size\": 256,\n",
        "        \"max_pos_n_embed\": 2048,\n",
        "        \"lr\": 2e-3,\n",
        "        \"n_layer\": 8,\n",
        "        \"n_head\": 16,\n",
        "        \"n_embed\": 64,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"eval_interval\": 200,\n",
        "        \"eval_steps\": 50,\n",
        "        \"n\": 1200000,\n",
        "        \"k\": 7999,\n",
        "        \"vocab_size\": 8000,\n",
        "    },\n",
        "    \"gpt-15M\":{\n",
        "        \"batch_size\": 64,\n",
        "        \"block_size\": 256,\n",
        "        \"max_pos_n_embed\": 2048,\n",
        "        \"lr\": 2e-3,\n",
        "        \"n_layer\": 8,\n",
        "        \"n_head\": 16,\n",
        "        \"n_embed\": 320,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"eval_interval\": 200,\n",
        "        \"eval_steps\": 50,\n",
        "        \"n\": 1200000,\n",
        "        \"k\": 7999,\n",
        "        \"vocab_size\": 8000,\n",
        "    },\n",
        "     \"tokenizer\":{\n",
        "        \"name\": \"EleutherAI/gpt-neo-125M\",\n",
        "    },\n",
        "    \"data\":{\n",
        "        \"name\": \"roneneldan/TinyStories\",\n",
        "    },\n",
        "}\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, Config(value))\n",
        "            else:\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.__dict__[key]\n",
        "\n",
        "config = Config(configdict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "D-1iBIRuNw1U"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, config, k=None, file_path=None, device=\"cpu\"):\n",
        "    self.k = k\n",
        "    self.file_path = file_path\n",
        "    self.device = device\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(config.name)\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    self.vocab_size = self.tokenizer.vocab_size if not self.k else self.k\n",
        "    self.initialize()\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"initl_vocab_size\": self.tokenizer.vocab_size,\n",
        "        \"final_vocab_size\": self.vocab_size,\n",
        "        \"vocab_size\": self.vocab_size,\n",
        "        \"total_tokens\": self.total_tokens,\n",
        "        \"total_tokens_used\": self.tokens_used if self.k else self.total_tokens,\n",
        "        \"total_unsed_tokens\": self.total_tokens - self.tokens_used if self.k else 0\n",
        "    }\n",
        "    return config\n",
        "\n",
        "  def initialize(self):\n",
        "    with open(self.file_path, 'r') as file:\n",
        "      tokens_counts = json.load(file)\n",
        "\n",
        "    self.total_tokens = sum(tokens_counts.values()) # Already sorted\n",
        "\n",
        "    if self.k:\n",
        "      self.tokens_used = sum([i for i in tokens_counts.values()][:self.k])\n",
        "      self.top_k_tokens = [i for i in tokens_counts.keys()][:self.k]# We will only use top k tokens, others will be ignored\n",
        "      self.top_k_tokens.append(\"50256\")\n",
        "      self.vocab_size +=1\n",
        "      self.top_k_tokens_dict =  {token: index for index, token in enumerate(self.top_k_tokens)}\n",
        "      self.reversed_top_k_tokens_dict = {value: int(key) for key, value in self.top_k_tokens_dict.items()}\n",
        "\n",
        "\n",
        "  def encoder(self, input, padding=False, max_length=256, truncation=False):\n",
        "    tokens = self.tokenizer(input , return_tensors='pt', padding=padding, max_length=max_length, truncation=truncation)['input_ids'].to(self.device)\n",
        "\n",
        "    if self.k:\n",
        "      tokens = torch.tensor([self.top_k_tokens_dict.get(str(token.item()), self.top_k_tokens_dict[\"50256\"]) for token in tokens.view(-1)], device=self.device).view(tokens.shape)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  def decoder(self, tokens):\n",
        "    if self.k:\n",
        "      tokens = torch.tensor([[self.reversed_top_k_tokens_dict[token.item()] for token in row] for row in tokens], device=tokens.device)\n",
        "\n",
        "    output = [self.tokenizer.decode(x, skip_special_tokens=True) for x in tokens]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Qs3xzFTyNjZH"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, config, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))  # (T, T)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, C)\n",
        "    q = self.query(x) # (B, T, C)\n",
        "    wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, C) X (B, C, T) --> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)  # (B,T,C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(config, head_size) for _ in range(config.n_head)])\n",
        "    self.proj  = nn.Linear(head_size * config.n_head, config.n_embed)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.concat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "   super().__init__()\n",
        "   self.layers = nn.Sequential(\n",
        "        nn.Linear(config.n_embed, 4 * config.n_embed),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * config.n_embed, config.n_embed),\n",
        "        nn.Dropout(config.dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    head_size = config.n_embed // config.n_head\n",
        "    self.sa_heads = MultiHeadAttention(config, head_size)\n",
        "    self.ffwd = FeedForward(config)\n",
        "    self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "    self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa_heads(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "  def __init__(self, config, device='cpu'):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.block_size = config.block_size\n",
        "    self.embedings = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "    self.position_embedings = nn.Embedding(config.max_pos_n_embed, config.n_embed)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "    self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "    self.ln_final = nn.LayerNorm(config.n_embed)\n",
        "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "  def save(self, path):\n",
        "    torch.save(self.state_dict(), path)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    token_embed = self.embedings(idx) # (B, T, C)\n",
        "    position_embed = self.position_embedings(torch.arange(T,  device=self.device)) # (T, C)\n",
        "    x = token_embed + position_embed # (B, T, C)\n",
        "    x = self.dropout(x) # (B, T, C)\n",
        "    x = self.blocks(x) # (B, T, C)\n",
        "    x = self.ln_final(x) # (B, T, C)\n",
        "    logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits = logits[..., :-1, :].contiguous()\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets[..., 1:].contiguous().view(-1), ignore_index=50256)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens, temperature=1.0, top_k=None):\n",
        "    # idx is (B, T)\n",
        "    for _ in range(max_tokens):\n",
        "      idx_cond = idx[:, -self.block_size:]\n",
        "      logits, _ = self(idx_cond) # (B, T, C)\n",
        "      logits = logits[:, -1, :]  / temperature # (B, C)\n",
        "      if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "      probs = F.softmax(logits, dim=-1) # Softmax Independently for C dim\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.concat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4t7FZGG0N43m"
      },
      "outputs": [],
      "source": [
        "def load_model(config, path, device='cpu'):\n",
        "    model = GPT2(config, device=device)\n",
        "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def clean_string(input_string):\n",
        "    cleaned_string = re.sub(r'[^\\w\\s.,]', '', input_string)\n",
        "    cleaned_string = cleaned_string.replace('\\n', '')\n",
        "    return cleaned_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dTN8yQU7OgyH"
      },
      "outputs": [],
      "source": [
        "model_name= \"gpt-1M\"\n",
        "path = \"model-1M-8k-3.pth\"\n",
        "model_config = config[model_name]\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tokenizer = Tokenizer(config.tokenizer, k=model_config.k, file_path=\"tokens.json\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oon2DerkPcpU",
        "outputId": "535d1327-235b-461e-9df7-f6dd7db5116f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ". was a weird day, the little boy named Tim. Tim wanted to become the same little boy with his mom. He asked his mom to help him to use the dangerousets.\n",
            "\n",
            "His mom said, \"Tim, you can play with me some squirrels and care of us. But it's just very nice to ask Mom and not have to win!\"\n",
            "\n",
            "Tim wanted to help his mom. He thought about one because one day was his favorite toy. He started to explore a game. He didn't know what to do. His mom smiled and said, \"That's not a good idea. You can play music instead.\"\n",
            "\n",
            "Tim was sad again, except for his favorite friend and he found him very well. He said, \"Thank you, mom! I heard my name.\" His mom smiled and said, \"Sure, Tim. I will take him dressed in life.\"\n",
            "\n",
            "Tim and his mom were happy at helping him. They set up the game with his\n",
            "Elon told his mommy were taking her in the garden. Grandma had a big cup of treats, and some apples. They saw lots of exciting snacks that said it was going to eat.\n",
            "\n",
            "\"Let's eat them,\" said Mama.\n",
            "\n",
            "They walked to the fridge and put some apple on fruit. \"Mum, we can be dinner!\" Mommy said. Daddy was so hungry and said, \"Ok, please!\" The girl and the mommy both enjoyed eating and ate the food.\n"
          ]
        }
      ],
      "source": [
        "model = load_model(model_config, path, device=device)\n",
        "\n",
        "unconditional = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "prompt = \"Elon told his mom\"\n",
        "\n",
        "output1 = model.generate(unconditional, max_tokens=200, temperature=1, top_k=None)\n",
        "output2 = model.generate(tokenizer.encoder(prompt), max_tokens=200, temperature=1, top_k=None)\n",
        "\n",
        "print(clean_string(tokenizer.decoder(output1)[0]))\n",
        "print(clean_string(tokenizer.decoder(output2)[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
