{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViFd1jdOULJa",
        "outputId": "d2f69371-3277-4617-f89e-e2abf8f8618d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=12.0.0 (from datasets)\n",
            "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 10.0.1\n",
            "    Uninstalling pyarrow-10.0.1:\n",
            "      Successfully uninstalled pyarrow-10.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 15.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.17.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-15.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8MX4VvOJNun6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VzI7szuBN_O6"
      },
      "outputs": [],
      "source": [
        "configdict= {\n",
        "    \"gpt-1M\":{\n",
        "        \"batch_size\": 64,\n",
        "        \"block_size\": 256,\n",
        "        \"max_pos_n_embed\": 2048,\n",
        "        \"lr\": 2e-3,\n",
        "        \"n_layer\": 8,\n",
        "        \"n_head\": 16,\n",
        "        \"n_embed\": 64,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"eval_interval\": 200,\n",
        "        \"eval_steps\": 50,\n",
        "        \"n\": 1200000,\n",
        "        \"k\": 7999,\n",
        "        \"vocab_size\": 8000,\n",
        "    },\n",
        "    \"gpt-15M\":{\n",
        "        \"batch_size\": 64,\n",
        "        \"block_size\": 256,\n",
        "        \"max_pos_n_embed\": 2048,\n",
        "        \"lr\": 2e-3,\n",
        "        \"n_layer\": 8,\n",
        "        \"n_head\": 16,\n",
        "        \"n_embed\": 320,\n",
        "        \"dropout\": 0.2,\n",
        "        \"epochs\": 1,\n",
        "        \"eval_interval\": 200,\n",
        "        \"eval_steps\": 50,\n",
        "        \"n\": 1200000,\n",
        "        \"k\": 7999,\n",
        "        \"vocab_size\": 8000,\n",
        "    },\n",
        "     \"tokenizer\":{\n",
        "        \"name\": \"EleutherAI/gpt-neo-125M\",\n",
        "    },\n",
        "    \"data\":{\n",
        "        \"name\": \"roneneldan/TinyStories\",\n",
        "    },\n",
        "}\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                setattr(self, key, Config(value))\n",
        "            else:\n",
        "                setattr(self, key, value)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return self.__dict__[key]\n",
        "\n",
        "config = Config(configdict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "D-1iBIRuNw1U"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, config, k=None, file_path=None, device=\"cpu\"):\n",
        "    self.k = k\n",
        "    self.file_path = file_path\n",
        "    self.device = device\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(config.name)\n",
        "    self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "    self.vocab_size = self.tokenizer.vocab_size if not self.k else self.k\n",
        "    self.initialize()\n",
        "\n",
        "  def get_config(self):\n",
        "    config = {\n",
        "        \"initl_vocab_size\": self.tokenizer.vocab_size,\n",
        "        \"final_vocab_size\": self.vocab_size,\n",
        "        \"vocab_size\": self.vocab_size,\n",
        "        \"total_tokens\": self.total_tokens,\n",
        "        \"total_tokens_used\": self.tokens_used if self.k else self.total_tokens,\n",
        "        \"total_unsed_tokens\": self.total_tokens - self.tokens_used if self.k else 0\n",
        "    }\n",
        "    return config\n",
        "\n",
        "  def initialize(self):\n",
        "    with open(self.file_path, 'r') as file:\n",
        "      tokens_counts = json.load(file)\n",
        "\n",
        "    self.total_tokens = sum(tokens_counts.values()) # Already sorted\n",
        "\n",
        "    if self.k:\n",
        "      self.tokens_used = sum([i for i in tokens_counts.values()][:self.k])\n",
        "      self.top_k_tokens = [i for i in tokens_counts.keys()][:self.k]# We will only use top k tokens, others will be ignored\n",
        "      self.top_k_tokens.append(\"50256\")\n",
        "      self.vocab_size +=1\n",
        "      self.top_k_tokens_dict =  {token: index for index, token in enumerate(self.top_k_tokens)}\n",
        "      self.reversed_top_k_tokens_dict = {value: int(key) for key, value in self.top_k_tokens_dict.items()}\n",
        "\n",
        "\n",
        "  def encoder(self, input, padding=False, max_length=256, truncation=False):\n",
        "    tokens = self.tokenizer(input , return_tensors='pt', padding=padding, max_length=max_length, truncation=truncation)['input_ids'].to(self.device)\n",
        "\n",
        "    if self.k:\n",
        "      tokens = torch.tensor([self.top_k_tokens_dict.get(str(token.item()), self.top_k_tokens_dict[\"50256\"]) for token in tokens.view(-1)], device=self.device).view(tokens.shape)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "  def decoder(self, tokens):\n",
        "    if self.k:\n",
        "      tokens = torch.tensor([[self.reversed_top_k_tokens_dict[token.item()] for token in row] for row in tokens], device=tokens.device)\n",
        "\n",
        "    output = [self.tokenizer.decode(x, skip_special_tokens=True) for x in tokens]\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Qs3xzFTyNjZH"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, config, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.query = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.value = nn.Linear(config.n_embed, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(config.block_size, config.block_size)))  # (T, T)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    k = self.key(x) # (B, T, C)\n",
        "    q = self.query(x) # (B, T, C)\n",
        "    wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 # (B, T, C) X (B, C, T) --> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)  # (B,T,C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, config, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(config, head_size) for _ in range(config.n_head)])\n",
        "    self.proj  = nn.Linear(head_size * config.n_head, config.n_embed)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = torch.concat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "   super().__init__()\n",
        "   self.layers = nn.Sequential(\n",
        "        nn.Linear(config.n_embed, 4 * config.n_embed),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(4 * config.n_embed, config.n_embed),\n",
        "        nn.Dropout(config.dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    head_size = config.n_embed // config.n_head\n",
        "    self.sa_heads = MultiHeadAttention(config, head_size)\n",
        "    self.ffwd = FeedForward(config)\n",
        "    self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "    self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa_heads(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "  def __init__(self, config, device='cpu'):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.block_size = config.block_size\n",
        "    self.embedings = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "    self.position_embedings = nn.Embedding(config.max_pos_n_embed, config.n_embed)\n",
        "    self.dropout = nn.Dropout(config.dropout)\n",
        "    self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "    self.ln_final = nn.LayerNorm(config.n_embed)\n",
        "    self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
        "\n",
        "  def get_parameters(self):\n",
        "    return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "  def save(self, path):\n",
        "    torch.save(self.state_dict(), path)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "    token_embed = self.embedings(idx) # (B, T, C)\n",
        "    position_embed = self.position_embedings(torch.arange(T,  device=self.device)) # (T, C)\n",
        "    x = token_embed + position_embed # (B, T, C)\n",
        "    x = self.dropout(x) # (B, T, C)\n",
        "    x = self.blocks(x) # (B, T, C)\n",
        "    x = self.ln_final(x) # (B, T, C)\n",
        "    logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      logits = logits[..., :-1, :].contiguous()\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets[..., 1:].contiguous().view(-1), ignore_index=50256)\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_tokens, temperature=1.0, top_k=None):\n",
        "    # idx is (B, T)\n",
        "    for _ in range(max_tokens):\n",
        "      idx_cond = idx[:, -self.block_size:]\n",
        "      logits, _ = self(idx_cond) # (B, T, C)\n",
        "      logits = logits[:, -1, :]  / temperature # (B, C)\n",
        "      if top_k is not None:\n",
        "        v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "        logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "      probs = F.softmax(logits, dim=-1) # Softmax Independently for C dim\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      idx = torch.concat((idx, idx_next), dim=1) # (B, T+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Te4z0L9_UIK8"
      },
      "outputs": [],
      "source": [
        "def load_data(config, batch_size, n, device='cpu'):\n",
        "    dataset = load_dataset(config.name)\n",
        "    train_data = DataLoader(dataset[\"train\"][:n][\"text\"], batch_size=batch_size, shuffle=True, pin_memory=True, pin_memory_device=device)\n",
        "    val_data = DataLoader(dataset[\"validation\"][:n][\"text\"], batch_size=batch_size, shuffle=True, pin_memory=True, pin_memory_device=device)\n",
        "\n",
        "    return train_data, val_data\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_data, val_data, encoder, eval_steps=50):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_steps)\n",
        "        for k in range(eval_steps):\n",
        "            data = train_data if split == 'train' else val_data\n",
        "            tokens = encoder(next(iter(data))[0], max_length=model.block_size, padding=\"max_length\", truncation=True)\n",
        "            _, loss = model(tokens, tokens)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "def plot_losses(losses):\n",
        "    train_losses = [o['train'] for o in losses if o.get('train') is not None]\n",
        "    valid_losses = [o['valid'] for o in losses if o.get('valid') is not None]\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(valid_losses, label='Validation Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Losses')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6TeolShMUDdF"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "  def __init__(self, config, model, optimizer, train_data, val_data, encoder):\n",
        "    self.config = config\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.train_data = train_data\n",
        "    self.val_data = val_data\n",
        "    self.encoder = encoder\n",
        "\n",
        "  def train(self, epochs, eval_interval=200, eval_steps=50):\n",
        "    max_steps = epochs * round(self.config.n / self.config.batch_size)\n",
        "    steps = 0\n",
        "    tracked_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      print(f\"Starting Epoch: {epoch + 1} {'-' * 100}\")\n",
        "      for batch in self.train_data:\n",
        "        if steps % eval_interval == 0 or steps == max_steps-1:\n",
        "          losses = estimate_loss(self.model, self.train_data, self.val_data, self.encoder, eval_steps)\n",
        "          tracked_losses.append(losses)\n",
        "          print(f\"Epoch: {epoch + 1}/{epochs} | Step: {steps}/{max_steps} | Train loss: {losses['train']:.4f} | Val loss: {losses['val']:.4f}\")\n",
        "\n",
        "        tokens = self.encoder(batch, max_length=self.config.block_size, padding=\"max_length\", truncation=True)\n",
        "        _, loss = self.model(tokens, tokens)\n",
        "        self.optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "    return tracked_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "bKE8fCpIUPhn",
        "outputId": "aa0cde46-197c-45f0-fb55-ca6d133bb3ed"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_name= \"gpt-1M\"\n",
        "model_config = config[model_name]\n",
        "\n",
        "train_data, val_data = load_data(config.data, model_config.batch_size, 64, device=device)\n",
        "tokenizer = Tokenizer(config.tokenizer, k=model_config.k, file_path=\"tokens.json\", device=device)\n",
        "\n",
        "model = GPT2(model_config, device=device)\n",
        "model = model.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=model_config.lr)\n",
        "\n",
        "trainer = Trainer(model_config, model, optim, train_data, val_data, tokenizer.encoder)\n",
        "tracked_losses = trainer.train(epochs=1, eval_interval=200, eval_steps=50)\n",
        "model.save(\"model-1M.bin\")\n",
        "\n",
        "print(tokenizer.get_config())\n",
        "plot_losses(tracked_losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
